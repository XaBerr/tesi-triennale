# Statistica Meccanica

## Introduzione
La fisica classica si basa sulla predizione perfetta degli eventi, per far ciò bisogna conoscere due elementi: la condizione iniziale e le leggi che regolano regolano l'evolizione del sistema. Un sistema chiuso è un sistema che o comprende tutto oppure è un sistema sufficentemente isolato da non avere perturbazioni dai sistemi esterni.
La meccanica statistica è la matematica statistica applicata ai sistemi fisici, viene utilizzata quando non si conoscono o le condizioni iniziali o le leggi che regolano il sistema con assoluta certezza o anche viene utilizzata in presenza di sistemi non chiusi.
Per esempio può essere utilizzata per studiare il comportamento delle molecole di gas contenute in una stanza. Lo studio della posizione e della quantità di moto di ogni singola particella all'interno della stanza è praticamente impossibile da ottenere e da gestire visto il numero elevato di molecole contenute. La meccanica statistica al contrario della sua collega diventa sempre più precisa con l'aumentare degli elementi presi in considerazione, ad esempio se uno conosce la temperatura può dedurne l'energia, la pressione. Sfortunatamente non si può dedurre tutto, ad esempio non si può dedurre il comportamento delle singole particelle, si può predirre la probabilità di una fluttuazione ma non quando questa avverrà.

## Ripasso di probabilità
Lo spazio di probabilità `i` può essere o uno spazio dei risultati di un esperimento oppure uno spazio degli stati di un sistema. Nel lancio di una moneta lo spazio può essere l'insieme `i = {testa, croce}`, nel lancio di una moneta può essere l'insieme delle faccie `i = {1,2,3,4,5,6}`. Consideriamo lo spazio `i = 1, ... n` dove `p(i)` è la probabilità associata al nostro sistema. La probabilità `p(i)` assume valori compresi tra `0` e `1`. La somma di tutte le `sum_i p(i) = 1`. Se si fa lo stesso esperimento un numero grande e si prendono i risultati degli esperimenti possiamo definire `N(i)` come il numero di volte in cui avviene l'evento `i`. Definiamo anche `N` come numero di volte in cui si è effettuato l'esperimento. Se `N` è sufficentemente grande (`N -> +\inf`)possiamo definire `p(i) = N(i)/N`. Definiamo `F(i)` come variabile aleatoria, ad esempio per il lancio di una moneta possiamo considerare `F(T) = 1` e `F(C) = -1`, possiamo considerare anche come variabile aleatoria altre cose ad esempio l'energia del sistema `E(i)`. Definizamo con `<F(i)>` il valore atteso della nostra variabile aleatoria `<F(i)> = sum_i F(i) p(i)` può anche essere visto come `<F(i)> = sum_i F(i) N(i) / N` per `N -> +\inf`.

## Lancio di un dado
Nel lancio di una dado la probabilità che esca una faccia è intuitivamente `p(x) = 1/6` questo perché il dado è simmetrico, la simmetria è uno dei legami fondamentali per l'equiprobabilità. Supponiamo che il dado non sia simmetrico allora uno dei modi che abbiamo per analizzare la probabilità è sperimentare. Si lancia il dado un numero significativo di volte, si registra la probabilità che esca ogni faccia.
Consideriamo un dado che funziona a stati, inizialmente parte da uno stato, successivamente attraverso certe regole può passare in un'altro stato. Per esempio potremmo avere un dado assimetrico che ha una regola di aggiornamento degli stati `1 -> 2 -> 3 -> 4 -> 5 -> 6 -> 1`. Supponendo ora il dado stia nello stesso stato un tempo fisso `t` possiamo ancora dedurre la probabilità di essere in uno stato non conoscendo la configurazione iniziale del sistema, in questo caso per simmetria dei tempi la probabilità continuerebbe a valere `1/6`. Possiamo ora considerare due cicli distinti per il nostro dado, il ciclo `a: 1 -> 2 -> 3 -> 1` e quello `b: 4 -> 5 -> 6 -> 4`. Se sappiamo di essere nel cliclo `a` allora la probabilità `p(4)` varrà `0`. La probabilità di essere nello stato `1` non sapendo qual'è il ciclo di partenza è `P(1) = p(a) * p(1)` dove `p(a)` è la probabilità di essere nel ciclo `a` e `p(1) = 1/3`. La legge di conservazione dice che le configurazioni di un sistema si possono rappresentare in cicli di stati come quelli appena descritti.

## Cattive leggi
Sono leggi che non sono presenti in natura, violano le leggi di conservazione dell'informazione. Ad esempio `1 -> 2, 2 -> 2, 3 -> 2, 4 -> 2, 5 -> 2, 6 -> 2`. Questa legge è considerata una legge cattiva perché si perde traccia degli stati passati del sistema. In natura questo tipo di legge non esiste, ma può capitare che durante una sperimentazione di perdere le traccie delle misurazioni. In quanto meccanica esiste un teorema che esprime l'analogo di questo concetto chiamato Liouville theorem. L'attrito può sembrare una cattiva legge perché da qualunque velocità parti tendi sempre a fermati, ma questo non è vero perché mentre ci si ferma si sta convertendo l'energia cinetica in energia di atrito (calore). Consideriamo ora l'equazione dell'atrito viscoso `d^2x_n / dt^2 = -\gamma * dx_n / dt`, questa legge è una legge correta ma sembra scorretta se applicata a tutte le molecole all'interno di una stanza perché non importa da che temperatura si parta ma alla fine la temperatura della stanza tenderà a diventare sempre `0`. Questo nella realtà non accade se si ha un sistema chiuso con una determinata temperatura (energia cinetica) esso la manterrà se non riceve perturabazioni.

## Entropia
Dato un'elenco di stati `N` dove ci sono `M < N` stati in cui il siste può essere con probabilità (ad esempio) `p_m = 1 / M`, `M` cresce con il crescere della nostra mancanza di informazione sul sistema. Se conoscessimo il sistema al 100% conosceremmo lo stato del sistema e quindi vincoleremmo `M = 1`. Se non conoscessimo niente del sistema allora lo stato portrebbe essere qualunque e quindi `M = N`. `S = log M` rappresenta l'entropia del sistema equiprobabile. L'entropia non dipende solo dal sistema ma anche dalla conoscenza che si ha dal sistema. Se continuiamo a seguire il sistema esso conserverà l'entropia, ma nella realtà non si riesce a seguire sepre il sistema e quindi l'entropia aumenta. La formula dell'entropia è `S = - sum_i p(i) log(p(i))` che rappresenta la media dell'informazione `log(p(i))`.

## Meccanica continua
Lo stato delle fasi consiste nel conoscere la posizione `x` e il momento `p` (massa * velocità). Consideriamo un'insieme del sottospazio delle fasi `Z` dove la probabilità di essere in un punto interno a `Z` è `p_Z = 1/|Z|` e vale `0` per ogni altro punto. Se il sistema evolve in `x` e `p` dopo un certo istante di tempo `t` esso si sarà spostato nello spazio `s X p` formando un nuovo insieme `Q`. Il teorema di Liouville dice che l'area di `Z` e l'area di `Q` coincidono. (lecture 1 time 1:02:35)
